{% block body %}

<div class="center">
<h2 class="mt-3">Deep Q Networks: Semi-Gradient vs Full-Gradient methods</h2>
<p class="lead">
     By Angelo Groot, Jordan Earle, Melle Vessies and Reitze Jansen.
</p>

<div class="row">
    <div class="col-sm-8 textbox">
        Deep Q Networks (DQN) are interesting deep reinforcement learning methods which leverage the flexibility of neural networks to handle reinforcement learning tasks which would be difficult or impossible to do tabularly due to high dimensional action-state space.
        The DQN turns the tabular value function into a Neural Network which uses the input state to produce the Q values for given actions as output.  One of the approaches used for DQN’s is the semi gradient update, which uses only part of the gradient in the update.
        This method allows for easier and faster computations, but does have the drawback of instability due to some of the shortcuts used.
        In order to understand how these drawbacks cause problems, what kind of environments cause these problems to manifest, and how the common tricks really fix these problems we have created a set of experiments to examine not only the vanilla DQN with semi gradient descent, but the common fixes in different environments.
        We will also attempt to find a network structure and methods which will solve all the environments well.
    </div>
</div>

<h2 class="mt-3">Q-learning</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        Before we get into Deep Q Networks it’s important to give a brief overview of Q learning.
        More in depth details can be found in Reinforcement Learning An Introduction [Sutton and Barto 2020].
        Central to Q learning is the Q function.
        The Q function is a function that maps every state-action pair to a value, which is determined by rewards given under the policy from this state to termination with some given discount factor.
        This can be seen in the equation below:
        <div class="formula">
            $$Q_{\pi} = \mathbb{E}[G_t | S_t = s, A_t = a ] = \mathbb{E}[\sum^{\infty}_{k = 0} \gamma^k R_{t + k + 1} | S_t = s, A_t = a]$$
        </div>
        This would allow us to then learn a value function for a given policy, but we can also learn the value function using monte carlo sampling.
        By using a given policy, we can calculate the Q function iteratively over an infinite number of episodes.
        In order to do this, we would need a policy which would give us a chance to explore each state, thus a policy such as epsilon policy.
        An epsilon policy states that a given state, you have an epsilon chance to take a random action instead of the policy action.
        This can also be done by assuming an epsilon greedy policy where we take the action of max value, or a random action with chance epsilon.
        By doing this we can learn the values of the Q function and create an epsilon greedy policy at the same time.
        This is the essence of Q learning.
        Q learning [Watkins, 1989] is a Reinforcement Learning algorithm from the Temporal Difference family of algorithms which allows for Off-Policy learning of a Q value function, which is then used to determine the policy by taking the greedy action given a state.
        This method is model free as it does not need the information from the environment to run.
        It just needs the rewards from the actions and an initial Q value function.
        Then for each step in the action chain, you update the Q value with the following update rule:
        <div class="formula">
            $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1}+\gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)] $$
        </div>

    </div>
</div>

<h2 class="mt-3">Deep Q-learning</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        here be text
    </div>

</div>

<h2 class="mt-3">Semi-Gradient vs Full-Gradient approach</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        here be text
    </div>
</div>

<h2 class="mt-3">Semi-Gradient vs Full-Gradient approach</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        here be text
    </div>
</div>


{% endblock %}
