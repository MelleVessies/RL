{% block body %}

<div class="center">
<h1 class="mt-3">Deep Q Networks analysis: divergence and problem solving.</h1>
<p class="lead">
     By Angelo Groot, Jordan Earle, Melle Vessies and Reitze Jansen.
</p>

<!-- <div class="row">
    <div class="line-graph" data-env="CartPole-v1"></div>
    <div class="heatmap" data-env="CartPole-v1" data-trick-id="Replay_10000__Target_False"></div>
</div> -->

<div class="row">
    <div class="col-sm-10 textbox">
        Deep Q Networks (DQN) is a family of deep Reinforcement Learning (RL) algorithms which leverage the flexibility
        of neural networks to handle reinforcement learning tasks which would be to difficult to solve
        tabularly because of their large state space. Instead of trying to learn a tabular q-function, a DQN tries to
        learn a continuous mapping from a state to a list of actions and their values. DQN is a popular architecture
        that can be used to solve a wide variety of tasks, however, they are far from perfect. A problem which DQN's suffer from
        is a phenomenon known in RL as the <a href="https://arxiv.org/abs/1812.02648">"deadly triad"</a>, which refers to
        reinforcement learning approaches where off-policy learning, function approximation and bootstrapping
        are used in tandem. The simultaneous use of these methods generally results
        in a highly unstable, volatile learning process which can cause divergence.
        As a result, many "tricks" have been constructed to stabilize the learning
        process such that we can still use these methods.

        We want to understand what kind of environments are prone to divergence with DQN's and how some of the common tricks can
        be used to mitigate these problems.  For this we have created a set of experiments to examine the performance
        of the DQN in addition to some of  the common fixes in different environments. We will also attempt to find
        a network structure and methods which will solve all the environments well.
    </div>
</div>

<h2 class="mt-3">Q-learning</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        Before we get into Deep Q Networks it’s important to give a brief overview of Q-learning.
        Q learning is an off policy method for learning the Q-value function <a href="http://incompleteideas.net/book/the-book-2nd.html"> (Sutton and Barto 2020) [2] </a> .
        An off policy method means that we have a behavioural policy to sample data from which is different
        from the target policy we are trying to learn.  In the case of Q learning, we are trying to learn a
        greedy policy. In off policy learning, we require a coverage assumption to be met for the behavioural
        policy.  This means that the behavioural policy must have a non zero probability to visit at least all
        the states that the target would.  Since at the start we do not know what the target policy should look like,
        we must have some policy which is able to visit all states.  This is generally called an $\epsilon$-soft policy.

        Central to Q learning is the Q function.  The Q function maps every state-action pair to a “quality” value,
        which indicates the expected (discounted) return as a result of performing that action in that state under
        the current policy.  The Q-value of a state-action pair under a policy $\pi$ is defined as the expected
        (discounted) return when performing the action in that state:

        <div class="formula">
            $$Q_{\pi}(S_t, A_t) = \mathbb{E}[G_t | S_t = s, A_t = a ] = \mathbb{E}[\sum^{\infty}_{k = 0} \gamma^k R_{t + k + 1} | S_t = s, A_t = a]$$
        </div>


        Where $Q_\pi(s, a)$ is the Q-value of state-action pair $s$ and $a$, $G_t$ is the (discounted) return at
        timestep $t$, $R_t$ is the observed reward at timestep $t$ and $\gamma$ is the discount factor.
        Q learning (<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.7501&rep=rep1&type=pdf">Watkins, 1989</a>) [3]
        is a Reinforcement Learning algorithm from the Temporal Difference family of
        algorithms.  In Temporal Difference (TD) algorithms we use a bootstrapping method where, instead of the return
        from an episode ($R$), the update is based on the difference in rewards between $n$ steps (in our case $n=1$).
        This allows us to improve our estimate every transition/step, rather than needing an entire episode.

        If our estimate of the expected return is correct, we would expect that the only difference between our
        current estimate and the estimate for the next step is the reward we would obtain. The TD error is a
        quantification of how much the estimated value differs from the true return, in other words, how good
        our prediction is.  Formally, the TD error is given by:

        <div class="formula">
            $$\delta_t = R_t + \gamma max_a Q_\pi(S_{t+1}, a) - Q_\pi(S_t, A_t)$$
        </div>

        Where $max_aQ(S_t, a)$ is the estimate of the expected return that we get after taking the estimated optimal
        action from state $S_t$ under a certain policy $\pi$.  If we then minimize this TD error we are thus
        effectively making our estimate more accurate.  This use of the max causes a maximization bias which means
        the network may systematically overestimate the value of state-action pairs.  As can be seen in the defintion,
        the TD error is a measurement of the predicted reward against the actual reward.   In Q learning these
        TD-errors are used to update the Q value estimates.   By taking a step in the direction of the difference
        between the real value of the reward and the estimated, we can closer approximate the true expected return.
        This can also be seen in the Q-learning update equation, since the difference between the return $G_t$ and
        $\gamma G_{t+1}$ should only be $R_{t+1}$.

        By following a behaviour policy which is also based on the Q-function, we can estimate/learn the Q-values
        iteratively while exploring acting in the environment . One such policy is $\epsilon$-greedy, which has a
        probability of $\epsilon$ to do a random action rather than the predicted optimal action. In this way the agent
        can learn what happens when performing the best actions according to the value function, while also still
        exploring different options.  By following this policy we can update our Q-values using the TD error as
        described above which gives rise to the following update rule:

        <div class="formula">
            $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1}+\gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)] $$
        </div>


    </div>
</div>

<h2 class="mt-3">Deep Q-learning</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        A Deep Q Network (DQN) uses the same principle as Q learning. The agent takes the action with the currently
        optimal value according to the current Q value function with some given $\epsilon$ chance of doing a random
        action, and the rewards from this action are used to update the Q value function.  In the case of DQN though,
        the Q value is no longer  represented as a table (a list of values corresponding to a state and action pair),
        but as a deep neural network function with the state as input and the output being a vector with the
        state-action values of possible actions. This allows us to have a much larger state and action space,
        since the values are represented by the network, which requires less space for the weights  than would be
        required for storing all of the state action pairs for complex problems.
    </div>
</div>
<div class="row">
    <div class="col-sm-4 textbox">
        The difference between the two can
        be seen in the image from <a href="https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python" >Ankit Choudhary </a>,
        showing how the network input is just the state where the Q-Learning input would be the state action pair,
        and the output will be a list of Q-Values corresponding to the actions, where in Q-learning we would have
        the output be the specific Q-Value for the state action pair input.

        <br/>
        <br/>

        In neural networks, updates to the weights are done through taking the gradient of the loss with respect
        to the weights.  In DQN, the loss is the TD error previously described, but due to the target in the DQN
        being the bootstrapped approximation for the Q function, taking the full gradient would cause problems
        due to the circular dependency.  Therefore, in practice we use the semi-gradient approach in which the
        gradient is simplified, using just part of the true gradient.   More information on the semi gradient
        approach can be seen in RL:AI[2020] and the semi gradient update can be written as:


        <div class="formula" id="dqn-update-formula">
            $$w_{t+1} = w_{t} + \alpha \left [ R_{t+1} + \text{max}_{a_{t+1}} (Q(s_{t+1}, a_{t+1}, w_{t})) - Q(s_{t}, a_{t}, w_{t}) \right ] \mathbf{\nabla}_{w}Q(s_{t}, a_{t}, w_{t})$$
        </div>


    </div>

    <br/>
    <br/>

    <div class="col-sm-8">
        <img width="600" src="static/img/benchmark RL(human level control))-rotated.png" alt="RL benchmark list">
    </div>

    <br/>
    <br/>

    <div class="col-sm-10 textbox">
        An example of the DQN working with breakout can be seen here:

        <br/><br/>



        <div style="text-align: center">
            <iframe
                    width="560"
                    height="315"
                    src="https://www.youtube.com/embed/TmPfTpjtdgg"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
            </iframe>
        </div>

        <br/>


        DQN is a powerful technique but it does require some tricks to work effectively. This is because the method has all the components of the deadly triad:

        <ul>
            <li>
                <b>Bootstrapping</b>:  Using the estimation of the value function in order to calculate the expected return for a single
                timestep rather than using the received return for the entire episode.  This can be seen in the update rule we are using.
            </li>
            <li>
                <b>Off Policy Learning</b>:  Learning from data generated by a different policy than the one you are learning.
                In this case we gather data using $\epsilon$-greedy to learn a greedy policy.
            </li>
            <li>
                <b>Function Approximation</b>: generalizing the statespace to some approximate domain, rather than learning (action)
                values for each distinct state, is the case for DQN as it does Q-learning where it represents the Q-values
                for all states in a single neural network
            </li>
        </ul>

        <a href="https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf"> Mnih et al.[2015] </a>
        combined the DQN with some tricks to prevent divergence and allow it to complete the complicated tasks.
        In this blog we’ll look at the concrete effect of using “experience replay” and “target networks”.
        In order to gain a deeper understanding into why these tricks prevent divergence and when they are helpful.

    </div>

</div>

<h2 class="mt-3">Environments</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        Environments in reinforcement learning refer to any space where agents interact to either get a task
        completed or to continually make good actions. This could be something as simple as learning which slot
        machine will give a good payoff, to putting a box from one conveyor belt to another or even as complex
        as a game of go.  Since each of these environments can differ greatly and they each bring their own
        difficulties.   We hypothesize that the reward landscape plays a large role in the chance of divergence
        for the DQN. In order to fully test what kind of problems can be seen when training a DQN, we chose 3
        environments with different reward landscapes in order to see how they would affect the convergence of
        the network.   For this, we have chosen 3 environments with very different reward landscapes: the Cartpole-v1,
        MountainCar-v0, and LunarLander-v2 environments from the open ai gym package.
    </div>
</div>

<div class="row">

    <div class="col-sm-5 textbox">
        <h3 class="mt-3">CartPole-v1</h3>

        In the CartPole problem, a pole is attached to an unarticulated joint on a cart, moving along a frictionless
        path.  The objective is to keep a pole upright, between a certain angle for as long as possible.
        A reward of +1 is provided at every time step the pole remains upright.  The episode ends when the pole falls
        below a 15 degree angle from the vertical axis, whenmoves 2.4 units from the center, or when 500 steps have
        passed.  The state the agent receives is  (cart position, cart velocity, pole angle, pole velocity at tip).
        The actions the agent has are applying a force to either push the car to the left or the right to the kart to
        keep the pole upright.
    </div>

    <div class="col-sm-7" style="padding-top: 100px">
        <video width="480" height="360" controls>
                    <source src="/static/videos/best_cartpole.mp4" type="video/mp4">
                        Your browser does not support the video tag.
        </video>
    </div>

    <div class="col-sm-10 textbox">
        This is an episodic environment as it does have an end of episode state, but if the agent is performing
        well, it could keep the pole upright for an indefinite amount of time.  This would cause the reward to keep
        growing.  The reward landscape here is uniform positive and the longer the cart keeps the pole upright, the
        more reward can be earned. Agents learning this environment likely tend towards short episodes at the start as the
        agent is taking random moves and has not learned how to keep the pole upright yet. As the agent learns, the rewards
        will get larger, meaning the DQN may start to update its weights more aggressively.
    </div>
</div>

<div class="row">

    <div class="col-sm-5" style="padding-top: 100px">
        <video  width="480" height="360" controls>
                    <source src="/static/videos/alright_mountaincar.mp4" type="video/mp4">
                        Your browser does not support the video tag.
        </video>
    </div>

    <div class="col-sm-5 textbox">
        <h3 class="mt-3">MountainCar-v0</h3>

        In the MountainCar problem, a car starts in between 2 mountains on a 1d track with the goal on the top of the mountain
        to the right. It is impossible for the car to get to the top of the mountain by just heading towards the goal,
        so it must first learn to go backwards to be able to gain enough speed to reach it's goal.  The reward for the
        problem is -1 for each time step, and the episode ends when the cart reaches the goal or when 500 steps have
        passed.  The states the agent receives are the (position, velocity) of the cart.  The actions the agent has is
        to either push the kart to the left, no push, or push the cart to the right.
    </div>


    <div class="col-sm-10 col-offset-2 textbox">
        This environment is also episodic as there is a clear end state.  The problem in this environment is the
        need for exploration at the start.  Since the kart has to go backwards first, it needs to remember that
        those backward steps were important.  The reward landscape for MountainCart is the inverse of Cartpole as
        it has a uniform negative landscape where every time step we haven't solved the problem we get an additional
        negative reward of -1.  This environment tends towards longer episodes at the start
        as the agent has not learned to first go backwards to gain momentum to go towards the goal, so it mostly
        times out. Only after it reaches its goal for the first time trough random exploration, it will truly start learning.
    </div>
</div>

<div class="row">

    <div class="col-sm-5 textbox">
        <h3 class="mt-3">LunarLander-v2</h3>

        In the lunar lander problem, an agent is attempting to land on a landing pad with 0 speed after being spawned
        at the top of the screen with changing landscapes. Landing on the landing pad, between the flags, gives a
        reward of +100 and doing so safely gives the agent an additional reward of +100. Crashing into the ground
        gives a negative reward (-100). Landing (anywhere) or crashing both end the episode. The agent also gets
        a bonus for landing safely, with +10 for each leg touching the ground and a quadratic penalty for its speed.
        The agent has 4 possible actions to its disposal  to achieve this.
    </div>

    <div class="col-sm-7" style="padding-top: 100px">
        <video width="480" height="360" controls>
                    <source src="/static/videos/best_lunar_lander.mp4" type="video/mp4">
                        Your browser does not support the video tag.
        </video>
    </div>

    <div class="col-sm-10 textbox">
        Doing nothing, firing one of its side
        engines or firing the main engine, applying force from the bottom of the agent upwards (whatever may be
        ‘up’ from the perspective of the agent).
        Firing the engines has a cost however, although the engine has
        infinite fuel, firing the main engine has a cost of 0.3 and firing any of the side engines a cost of 0.03.
        The environment is considered solved when the agent reaches a return of 200 which happens when landing with
        low velicity on the pad while spending little fuel.
    </div>
</div>




<h2 class="mt-3">Experimental Setup</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        Comparing these methods should be done in a way that has an equal playing field. Since the tricks change how
        the agent learns and what it uses to learn, it might be the case that one setting of hyperparameters favours
        the performance of some methods over others. To fairly compare the impact of tricks, we will first establish
        baselines for each method. To establish this baseline we do a grid search for each of the environments, for
        each of the methods and investigate how they impact divergence when each method is at their best.  to compare
        to and determine how much the tricks help to improve the divergence, if it occurs.
        Now that the environments have been selected for their diverse reward landscapes, we can set up experiments
        to evaluate  the performance (Return) and divergence (growth in Mean Squared TD errors) of the models.
        As <a href="https://arxiv.org/pdf/1709.06560.pdf"> Henderson et. al</a> find, different algorithms have
        different hyperparameters where they perform well, as such to make an at least somewhat fair comparison where
        the model hyperparameters are not the (sole) reason for the divergence,  we will be doing a grid search over
        different values for the discount factor $\gamma$ and the exploration probability ($\epsilon$).
        In order to more robustly evaluate the effect of hyperparameters, we train the models on 5 different random
        seeds (1-5) and evaluate the target policy from each trained model on 10 different test seeds (100-109). Then
        we compare the training divergence and test performance of best found hyperparameters.  In order to help the
        convergence of the system, we will implement the 2 tricks previously mentioned, the experience replay and the
        fixing of the target network.  These tricks will be evaluated in the same manner as discussed above and all
        the results will be compared.  Let's first take a quick look at each trick in more detail.
    </div>
</div>




<h2 class="mt-3">Experience Replay</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        Now, let us say the agent tries to become good at whatever task is given by changing its actions according
        to what it encounters in its ventures, using the <a href="#dqn-update-formula">above update rule</a> to update the parameters
        that determine its estimated Q-values. It could do so whenever it gets a new reward (or whenever it gets enough
        data to perform a batch update) but this means the agent continually only is adjusting to what it has
        encountered most recently. This problem was - according to
        <a href="http://incompleteideas.net/book/the-book-2nd.html"> Barto and Sutton </a> [2] - first described by
        <a href="http://www.incompleteideas.net/lin-92.pdf"> Long-Ji Lin in 1992</a> [4]. He made the observation that
        it might be wasteful to throw away experiences as some might be rare or costly to obtain in situations with
        physical agents. As a result he proposed to reuse experiences, which he defines as a pairing of state, the
        action taken, the reinforcement (reward) obtained and the following state. He calls reusing past experiences
        Experience Replay. In the paper by Lin the 100 most recent episodes (lessons) are sampled randomly,
        with recent experiences being exponentially more likely. However, more recently the approach has been to
        either sample experiences uniformly with recency governed by the size of a replay buffer
        ( <a href="https://arxiv.org/pdf/1312.5602.pdf?"> see this paper for example</a> [6]) or by using
        <a href="https://arxiv.org/pdf/1511.05952.pdf "> prioritization </a> [7] to determine which experiences should
        be replayed in this blog we will look at whether standard (uniform sampling) experience replay can improve the
        performance of DQNs through reducing divergence.
    </div>
</div>

<h2 class="mt-3">Target Network Fixing</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        In order to help with the problem of the target (TD error) containing what we are updating (the predicted Q
        function from bootstrapping to replace the true expected return), the network is fixed at a certain time and is
        used as the target for the TD error.  This fixed network is called the target network and allows for a more
        stable update by ensuring the difference between the two Q functions is larger, so the network has a more
        accurate direction to update in.  This can be seen as a dog chasing its own tail, getting faster and faster
        trying to catch it but never catching up.  The target network, in combination with the experience replay, is
        what made the DQN in the
        <a href="https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf">
            paper by Mnih et. al. (2015)</a>  perform well on the Arati games.

        The target network in our implantation starts at 0 updates and then every 2 episodes.  This
        is because we want the network to be dependent on more recent samples than those seen more than 2 episodes prior.

    </div>
</div>

<h2 class="mt-3">Results and Discussion</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        In this section we will be looking at the results of the grid-”search”, which show the average final episode returns
        and the performance for each of the trick settings over episodes. To show this we show the mean accross 5 seeds
        and one fifth of their standard deviations. The grid search results are organised as follows: Left) No tricks applied - Center
        left) Using a target network, - Center right) Using experience replay -  Right) both a target network and
        experience replay.

</div>
</div>

<h3 class="mt-3">Cartpole-v1</h3>
<div class="row">
    <div class="col-sm-8 textbox">
        We will start by examining the Cartpole environment.  As previously mentioned, the environment will
        tend to have shorter episodes at the start while agents are learning.  It has a positive uniform landscape,
        as each step the pole stays upright gives +1 reward.  The heatmaps for the vanilla DQN, the two tricks and
        the combination of tricks can be seen in the figures below.
    </div>
</div>
<div class="row expanded">
    <div class="col-sm-2 offset-sm-1">
        <div class="heatmap" data-env="CartPole-v1" data-trick-id="no_tricks"></div>
    </div>
    <div class="col-sm-2">
        <div class="heatmap" data-env="CartPole-v1" data-trick-id="target_network_fixing"></div>
    </div>
    <div class="col-sm-2">
        <div class="heatmap" data-env="CartPole-v1" data-trick-id="experience_replay"></div>
    </div>
    <div class="col-sm-3">
        <div class="heatmap" data-env="CartPole-v1" data-trick-id="both_tricks" data-plot-legend="true"></div>
    </div>
</div>

<b>Divergence: </b>

<!-- AYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY -->

<div class="row expanded">
    <div class="col-sm-2 offset-sm-1">
        <div class="divergence_heatmap" data-env="CartPole-v1" data-trick-id="no_tricks"></div>
    </div>
    <div class="col-sm-2">
        <div class="divergence_heatmap" data-env="CartPole-v1" data-trick-id="target_network_fixing"></div>
    </div>
    <div class="col-sm-2">
        <div class="divergence_heatmap" data-env="CartPole-v1" data-trick-id="experience_replay"></div>
    </div>
    <div class="col-sm-3">
        <div class="divergence_heatmap" data-env="CartPole-v1" data-trick-id="both_tricks" data-plot-legend="true"></div>
</div>
</div>

<!-- AYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY -->


<div class="row">
    <div class="col-sm-6 textbox">

        This shows that to solve the Cartpole problem it is very important to include experience replay, as without it
        (in the left and center-left heatmaps) the agent is not able to perform the task well for any hyperparameter
        setting. With Just experience replay, we see that performance improves for approximately the “lower half” of
        the discount factors, and improves for performance for the higher discount factors when the target network is
        combined with experience replay. Now lets see if we can find the culprit..
        When we take a further look and see how each of the trick applications performs over time we notice that the
        baseline without tricks and using the tarket network immediately decrease in performance, even to below their
        random initialization go on a race to the bottom. Furthermore, even with experience replay (and the target
        network combined) even after learning well during the first 50 episodes, their performances decrease.
    </div>
    <div class="col-sm-6">
        <div class="line-graph" data-env="CartPole-v1"></div>
    </div>
</div>


<h3 class="mt-3">Mountaincar-v0</h3>
<div class="row">
    <div class="col-sm-10 textbox">

        Mountaincar-v0
        Next we take a look at the Mountaincar environment. This environment will tend to have longer episodes as the
        model needs to figure out that it has to go backwards first to gain momentum before going forwards.  This
        environment is a negative uniform landscape, as each step the model hasn't reached the goal, it gains -1 reward.
    </div>
</div>
<div class="row expanded">
    <div class="col-sm-2 offset-sm-1">
        <div class="heatmap" data-env="MountainCar-v0" data-trick-id="no_tricks"></div>
    </div>
    <div class="col-sm-2">
        <div class="heatmap" data-env="MountainCar-v0" data-trick-id="target_network_fixing"></div>
    </div>
    <div class="col-sm-2">
        <div class="heatmap" data-env="MountainCar-v0" data-trick-id="experience_replay"></div>
    </div>
    <div class="col-sm-3">
        <div class="heatmap" data-env="MountainCar-v0" data-trick-id="both_tricks" data-plot-legend="true"></div>
</div>
</div>
<b>Divergence: </b>

<!-- AYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY -->

<div class="row expanded">
    <div class="col-sm-2 offset-sm-1">
        <div class="divergence_heatmap" data-env="MountainCar-v0" data-trick-id="no_tricks"></div>
    </div>
    <div class="col-sm-2">
        <div class="divergence_heatmap" data-env="MountainCar-v0" data-trick-id="target_network_fixing"></div>
    </div>
    <div class="col-sm-2">
        <div class="divergence_heatmap" data-env="MountainCar-v0" data-trick-id="experience_replay"></div>
    </div>
    <div class="col-sm-3">
        <div class="divergence_heatmap" data-env="MountainCar-v0" data-trick-id="both_tricks" data-plot-legend="true"></div>
</div>
</div>

<!-- AYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY -->

<div class="row">
    <div class="col-sm-6">
        <div class="line-graph" data-env="MountainCar-v0"></div>
    </div>
    <div class="col-sm-6 textbox">
          The heatmaps for the model setups can be seen in the figures above and show similar results as the cartpole
        setting. There are however, two differences. Firstly there seems to be a more narrow range of hyperparameters
        for reliable good performance for this problem. Secondly, using a target network potentially does have *some*
        impact here, although its impact is not significant enough to draw any further conclusions. When looking at the
        improvement over episodes we can see that on average convergence has not been reached yet. This means that, to
        further explore the divergence characteristics of this environment, we should in future experiments train longer.
    </div>
</div>

<h3 class="mt-3">LunarLander-v2</h3>
<div class="row">
    <div class="col-sm-10 textbox">
        Finally we take a look at the LunarLander environment. This environment will tend to have shorter episodes
        at the start as it tends to crash. It has a much more complicated rewards landscape, with negative
        reward for the actions it takes and any time it crashes, but positive rewards for different types of landings
        as previously mentioned.
    </div>
</div>

<div class="row expanded">
<div class="col-sm-2 offset-sm-1">
    <div class="heatmap" data-env="LunarLander-v2" data-trick-id="no_tricks"></div>
</div>
<div class="col-sm-2">
    <div class="heatmap" data-env="LunarLander-v2" data-trick-id="target_network_fixing"></div>
</div>
<div class="col-sm-2">
    <div class="heatmap" data-env="LunarLander-v2" data-trick-id="experience_replay"></div>
</div>
<div class="col-sm-3">
    <div class="heatmap" data-env="LunarLander-v2" data-trick-id="both_tricks" data-plot-legend="true"></div>
</div>
</div>

<b>Divergence: </b>

<!-- AYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY -->

<div class="row expanded">
    <div class="col-sm-2 offset-sm-1">
        <div class="divergence_heatmap" data-env="LunarLander-v2" data-trick-id="no_tricks"></div>
    </div>
    <div class="col-sm-2">
        <div class="divergence_heatmap" data-env="LunarLander-v2" data-trick-id="target_network_fixing"></div>
    </div>
    <div class="col-sm-2">
        <div class="divergence_heatmap" data-env="LunarLander-v2" data-trick-id="experience_replay"></div>
    </div>
    <div class="col-sm-3">
        <div class="divergence_heatmap" data-env="LunarLander-v2" data-trick-id="both_tricks" data-plot-legend="true"></div>
</div>
</div>

<!-- AYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY -->


<div class="row">
    <div class="col-sm-6 textbox">

        The heatmaps for the model setups can be seen in figures above and seem to show relatively good performance for
        all hyperparameters, with an emphasis for lower epsilon values when experience replay is used. This result was
        initially surprising, as this environment was selected for its presumed more complex reward landscape.

        When we look at the performance over time we do see experience replay is again the factor which gives a major
        improvement, but this time mostly in how quickly the agent picks up on what to do. A point of interest, though,
        is that when the replay memory is introduced, the agent stops performing well for a discount factor of
        $\gamma$ = 1.
    </div>
<div class="col-sm-6">
    <div class="line-graph" data-env="LunarLander-v2"></div>
</div>
</div>


<h2 class="mt-3">Conclusion</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        Discuss what we saw, restate the mission statement and the results surrounding it.  Give a statement about
        what we want to do next? Refer back to the hypothesis about types of environments and how the different tricks
        help in them.

        Firstly, we have learned that it may not be obvious what qualifies as a “complex” reward landscape. Secondly,
        it seems that including experience replay during training has a positive effect w.r.t. Performance. This last
        point might be under the excluding of situations where no discoutn factor is applied, as we have observed that
        under this condition experience replay is detrimental for some environments (LunarLander).

        [Conclusions divergence]

    </div>
</div>

<h2 class="mt-3">References</h2>
<div class="row">
    <div class="col-sm-10 textbox">
        <ol>
            <li><a href="https://arxiv.org/abs/1812.02648"> Van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., & Modayil, J. (2018). Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648. </a></li>
            <li> <a href="http://incompleteideas.net/book/the-book-2nd.html"> Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.</a> </li>
            <li> <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.7501&rep=rep1&type=pdf"> Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning, 8(3-4), 279-292.</a>
            <li><a href="http://www.incompleteideas.net/lin-92.pdf"> Lin, L. J. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4), 293-321. </a></li>
            <li><a href="https://gym.openai.com/"> OpenAI Gym </a></li>
            <li><a href="https://arxiv.org/pdf/1312.5602.pdf?"> Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602. </a></li>
            <li><a href="https://arxiv.org/pdf/1511.05952.pdf "> Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952. </a></li>
            <li><a href="https://arxiv.org/pdf/1709.06560.pdf"> Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560. </a></li>
            <li><a href="https://github.com/pytorch/pytorch"> https://github.com/pytorch </a></li>
            <li></li>
            <li></li>
        </ol>
    </div>
</div>

<script type="application/javascript">
    collect_graphs();
</script>

{% endblock %}
