{% block body %}

<div class="center">
<h2 class="mt-3">Deep Q Networks: Semi-Gradient vs Full-Gradient methods</h2>
<p class="lead">
     By Angelo Groot, Jordan Earle, Melle Vessies and Reitze Jansen.
</p>

<div class="row">
    <div class="col-sm-8 textbox">
        Deep Q Networks (DQN) are interesting deep reinforcement learning methods which leverage the flexibility of neural networks to handle reinforcement learning tasks which would be difficult or impossible to do tabularly due to high dimensional action-state space.
        The DQN turns the tabular value function into a Neural Network which uses the input state to produce the Q values for given actions as output.  One of the approaches used for DQN’s is the semi gradient update, which uses only part of the gradient in the update.
        This method allows for easier and faster computations, but does have the drawback of instability due to some of the shortcuts used.
        In order to understand how these drawbacks cause problems, what kind of environments cause these problems to manifest, and how the common tricks really fix these problems we have created a set of experiments to examine not only the vanilla DQN with semi gradient descent, but the common fixes in different environments.
        We will also attempt to find a network structure and methods which will solve all the environments well.
    </div>
</div>
<h2 class="mt-3">Q-learning</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        Before we get into Deep Q Networks it’s important to give a brief overview of Q learning.
        More in depth details can be found in Reinforcement Learning An Introduction [Sutton and Barto 2020].
        Central to Q learning is the Q function.
        The Q function is a function that maps every state-action pair to a value, which is determined by rewards given under the policy from this state to termination with some given discount factor.
        This can be seen in the equation below:
        $$ Q_\pi =\mathcal{Ε}[G_t | S_t = s, A_t = a] = \mathcal{Ε}[k = \sum_0^\infty k R_{t+k+1} | S_t = s, A_t = a] $$
    </div>
</div>

{% endblock %}
