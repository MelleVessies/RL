{% block body %}

<div class="center">
<h2 class="mt-3">Deep Q Networks: Semi-Gradient vs Full-Gradient methods</h2>
<p class="lead">
     By Angelo Groot, Jordan Earle, Melle Vessies and Reitze Jansen.
</p>

<div class="row">
    <div class="col-sm-8 textbox">
        Deep Q Networks (DQN) are interesting deep reinforcement learning methods which leverage the flexibility of neural networks to handle reinforcement learning tasks which would be difficult or impossible to do tabularly due to high dimensional action-state space.
        The DQN turns the tabular value function into a Neural Network which uses the input state to produce the Q values for given state-actions as output.  One of the approaches used for DQN’s is the semi gradient update, which uses only part of the gradient in the update.
        This method allows for easier and faster computations, but does have the drawback of instability due to some of the shortcuts used.
        In order to understand how these drawbacks cause divergence, what kind of environmental properties cause divergence to manifest, and how the common tricks really fix these problems we have created a set of experiments to examine not only the vanilla DQN with semi gradient descent, but the common fixes in different environments.
        We will also attempt to find a network structure and methods which will solve all the environments well.
    </div>
</div>

<h2 class="mt-3">Q-learning</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        Before we get into Deep Q Networks, it is important to give a brief overview of Q learning (in-depth details can be found in <i>Reinforcement Learning An Introduction</i> [Sutton and Barto 2020]).
        Central to Q learning is the Q function.
        The Q function is a function that maps every state-action pair to a value, which is determined by rewards given under the policy from this state to termination with some given discount factor.
        This can be seen in the equation below:
        <div class="formula">
            $$Q_{\pi}(S_t, A_t) = \mathbb{E}[G_t | S_t = s, A_t = a ] = \mathbb{E}[\sum^{\infty}_{k = 0} \gamma^k R_{t + k + 1} | S_t = s, A_t = a]$$
        </div>
        This would allow us to then determine a Q-value function for a given policy, but we can also learn the Q-value function using monte carlo sampling.
        By using a given policy, we can calculate the Q function iteratively over an infinite number of episodes.
        In order to do this, we would need a policy which would give us a chance to explore each state, thus a policy such as epsilon policy.
        An epsilon policy states that a given state, you have an epsilon chance to take a random action instead of the policy action.
        This can also be done by assuming an epsilon greedy policy where we take the action of max Q-value, or a random action with chance epsilon.
        By doing this we can learn the values of the Q function and create an epsilon greedy policy at the same time.
        This is the essence of Q learning.
        Q learning [Watkins, 1989] is a Reinforcement Learning algorithm from the Temporal Difference family of algorithms which allows for Off-Policy learning of a Q-value function, which is then used to determine the policy by taking the greedy action given a state.
        This method is model free as it does not need the information from the environment to run.
        It just needs the rewards from the actions and an initial Q-value function.
        Then for each step in the action chain, you update the Q-value with the following update rule:
        <div class="formula">
            $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1}+\gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)] $$
        </div>
        Where $S_t$ is the current state, $A_t$ is the current action, the reward for the current state action pair is $R_{t+1}$, $\gamma$ is a discount factor and $a$ is the estimated highest valued action for the next state.
        There are already inherent problems with Q-Learning at this point.  The main one being the maximization bias which is caused by taking the argmax in the update step.  It causes the algorithm to systematically overestimate the value of certain states over others, especially with limited data.
    </div>
</div>

<h2 class="mt-3">Deep Q-learning</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        Although the aforementioned method of using Q-learning can be used to iteratively estimate the value of state-action pairs, it does not deal well with continuous state spaces.
        This originates mostly from the sampling data-scarcity introduced by the continuous properties (i.e in a continuous space, the probability of observing exactly the same values multiple times is negligable).
        A way to deal with this is by using a neural network to learn and incorporate how the observed values of the states correlate and map it to an estimation of the Q-value for the actions.
        We refer to this neural network which outputs estimates of Q-values as the Deep Q Network (DQN)
        The power of the DQN can be seen in the paper by Mnih et al.[2015], where they train DQN’s to complete many of the old Atari games.  It was impressive to see it be able to discover longer term strategies and sequences, such as in breakout where it discovered the side tunnel strategy.  The performance of the DQN compared to the best linear learner can be seen in the following figure from the paper:

        <br/>
        <br/>

        <img src="static/img/benchmark RL(human level control))-rotated.png" alt="RL benchmark list">

        <br/><br/>

        An example of the DQN working with breakout can be seen here:

        <br/><br/>

        <div>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/TmPfTpjtdgg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>

        <br/>

        The DQN in this paper used tricks like replay memory and batching to allow it to complete the complicated tasks.
        <br><br/>
        Our goal will be to explore when methods like this are needed and to discuss why in order to have a better grasp of when to apply them in the future.
        In order to determine when a network was diverging, we first had to make sure we had a working network.  To do this we tried the problem with ____________ with 10 random seeds to train the hyperparameters, and to ensure that there was a solution to be found without a lucky or unlucky seed/initialization.  This was done using a grid search and the results can be seen below.  These seeds will be used for future hyperparameter tuning and searching to ensure all algorithms receive the same chance for success.
        <br><br>

        Seeing that the architecture worked, we had a starting point for testing other environments.  From the given algorithm and the shortcuts it uses (WE MIGHT WANT TO DISCUSS THESE SHORTCUTS ABOVE SOME MORE) we can already see some areas where we might encounter problems.  The first is with the maximization bias due to the max function.  Another area would cause problems is with the experiences being used only once.  Since some states might be hard to get into and we want to get the most out of the previous experience, it is inefficient and harder to converge if we only use each sample once.  Another place that could cause error is when an outlier is present, and the gradient from this causes a large change in the weights, leading to a much different Q function.  Using bootstrapping to estimate the future rewards is also likely to cause problems, since this could lead to an increase in variance if the discount factor is too high, or bias if the discount factor used is too low.  The final place that can cause problems that we are looking at today is the target in the semi gradient method is the same network we are updating through the weight change.  This can lead to a phenomenon referred to as catastrophic forgetting.  This is caused by the weights being updated for the whole Q function rather than just for the one state action pair.  Meaning that some outcome can have effects for states other than the one currently being updated.  This can compound and cause major problems.

    </div>

</div>

<h2 class="mt-3">Semi-Gradient vs Full-Gradient approach</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        This we dont need now???
    </div>
</div>

<h2 class="mt-3">Divergence and Problem Solving</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        In order to see under what conditions the semi gradient and full gradient approach work well (diverge?), we will be comparing the two algorithms on 4 separate gym environments (Acrobot, MountainCar, CartPole, LunarLander).
        In addition, we will be testing when certain tricks can help improve performance.
        <br>...
        Here be text
    </div>
</div>

<h2 class="mt-3">Experience Replay</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        here be text (illustrating what experience replay is and how we implement it)
    </div>
</div>

<h2 class="mt-3">Target Network Fixing</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        here be text (illustrating what Target Network fixing is and how we implement it)
    </div>
</div>

<h2 class="mt-3">Double Q Learning</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        here be text (illustrating what Double Q learning is and how we implement it)
    </div>
</div>

<h2 class="mt-3">Batching</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        here be text (illustrating why batching matters and how we implement it)
    </div>
</div>

<h2 class="mt-3">References</h2>
<div class="row">
    <div class="col-sm-8 textbox">
        1. Sutton, R. A., and Barto A Reinforcement Learning An Introduction 2020
        2. 
    </div>
</div>

{% endblock %}
